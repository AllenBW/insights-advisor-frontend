{
    "content_html": "<p>Actions identified with a medium level of risk</p>\n",
    "summary_html": "<p>Actions identified with a medium level of risk</p>\n",
    "rules": [
        {
            "rule_id": "xinetd_per_source_limit|XINETD_PER_SOURCE_LIMIT",
            "description": "Services controlled by xinetd fail to respond when  per_source_limit of xinetd is reached",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 1,
            "summary": "A large number of \"xinetd per_source_limit\" messages are being reported because services are reaching limits defined in the xinetd configuration file.",
            "summary_html": "<p>A large number of &quot;xinetd per_source_limit&quot; messages are being reported because services are reaching limits defined in the xinetd configuration file.</p>\n",
            "plugin": "xinetd_per_source_limit",
            "error_key": "XINETD_PER_SOURCE_LIMIT",
            "plugin_name": "Services controlled by xinetd fail to respond when  per_source_limit of xinetd is reached",
            "ansible": 0,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "xen_pv_guest_boot_failure|XEN_PV_GUEST_BOOT_FAILURE",
            "description": "Xen PV instance fails to boot with specific versions of kernel",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Xen PV instance fails to boot with kernel 2.6.32-696.18.7.el6 after kernel is upgraded to this version.\n",
            "summary_html": "<p>Xen PV instance fails to boot with kernel 2.6.32-696.18.7.el6 after kernel is upgraded to this version.</p>\n",
            "plugin": "xen_pv_guest_boot_failure",
            "error_key": "XEN_PV_GUEST_BOOT_FAILURE",
            "plugin_name": "Xen PV instance fails to boot with specific versions of kernel",
            "ansible": 1,
            "rec_impact": 3,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "xen_kdump_supported|XEN_KDUMP_SUPPORTED",
            "description": "Kdump does not work due to XEN/AWS's limitation.",
            "category": "Availability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Xen-based environments do not currently support `kdump`.\n",
            "summary_html": "<p>Xen-based environments do not currently support <code>kdump</code>.</p>\n",
            "plugin": "xen_kdump_supported",
            "error_key": "XEN_KDUMP_SUPPORTED",
            "plugin_name": "Kdump does not work due to XEN/AWS's limitation.",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 1,
            "acked": false
        },
        {
            "rule_id": "vpd_rw_failed|VPD_RW_FAILED_KERNEL",
            "description": "System lockups when abnormal VPD data returned from HBA due to bug in kernel",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "System lockups or VPD R/W failed messages can occur if the kernel reads past end VPD data tags or attempts to read any past invalid VPD tags that it finds.\n",
            "summary_html": "<p>System lockups or VPD R/W failed messages can occur if the kernel reads past end VPD data tags or attempts to read any past invalid VPD tags that it finds.</p>\n",
            "plugin": "vpd_rw_failed",
            "error_key": "VPD_RW_FAILED_KERNEL",
            "plugin_name": "System lockups or VPD R/W failed messages can occur when reading VPD data abnormally",
            "ansible": 1,
            "rec_impact": 3,
            "rec_likelihood": 1,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "vpd_rw_failed|VPD_RW_FAILED_FIRMWARE",
            "description": "System lockups when abnormal VPD data returned from HBA firmware",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "VPD error message occurs when running a QLogic HBA card with outdated firmware.",
            "summary_html": "<p>VPD error message occurs when running a QLogic HBA card with outdated firmware.</p>\n",
            "plugin": "vpd_rw_failed",
            "error_key": "VPD_RW_FAILED_FIRMWARE",
            "plugin_name": "System lockups or VPD R/W failed messages can occur when reading VPD data abnormally",
            "ansible": 0,
            "rec_impact": 3,
            "rec_likelihood": 2,
            "resolution_risk": 1,
            "acked": false
        },
        {
            "rule_id": "vm_swappiness_oom|VM_SWAPPINESS_OOM_ISSUE_WARN",
            "description": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions where database is running",
            "category": "Performance",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.\n",
            "summary_html": "<p>Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.</p>\n",
            "plugin": "vm_swappiness_oom",
            "error_key": "VM_SWAPPINESS_OOM_ISSUE_WARN",
            "plugin_name": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "vm_swappiness_oom|VM_SWAPPINESS_OOM_ISSUE_INFO",
            "description": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions",
            "category": "Performance",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.\n",
            "summary_html": "<p>Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions.</p>\n",
            "plugin": "vm_swappiness_oom",
            "error_key": "VM_SWAPPINESS_OOM_ISSUE_INFO",
            "plugin_name": "Out-of-Memory occurs when vm.swappiness is 0 in specific RHEL6 versions",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "vmw_driver_issue|VMW_DRIVER_ISSUE",
            "description": "VM guest with vmw_pvscsi driver can crash when using the specific kernel versions on VMware ESXi 5 platform",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "VM guests with the `vmw_pvscsi` paravirtualized driver can crash when using the kernel version prior to `kernel-2.6.32-504.el6.x86_64` on VMware ESXi 5 platform.\n",
            "summary_html": "<p>VM guests with the <code>vmw_pvscsi</code> paravirtualized driver can crash when using the kernel version prior to <code>kernel-2.6.32-504.el6.x86_64</code> on VMware ESXi 5 platform.</p>\n",
            "plugin": "vmw_driver_issue",
            "error_key": "VMW_DRIVER_ISSUE",
            "plugin_name": "VM guest with vmw_pvscsi driver can crash when using the specific kernel versions on VMware ESXi 5 platform",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "vmware_slabinfo_nmi_crash|VMWARE_SLABINFO_NMI_CRASH",
            "description": "NMI enabled on VMWare guest may cause system crash",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Guest virtual machines running under VMware can be crashed by the NMI watchdog when a process accesses `/proc/slabinfo` if the host system has been under heavy memory pressure.",
            "summary_html": "<p>Guest virtual machines running under VMware can be crashed by the NMI watchdog when a process accesses <code>/proc/slabinfo</code> if the host system has been under heavy memory pressure.</p>\n",
            "plugin": "vmware_slabinfo_nmi_crash",
            "error_key": "VMWARE_SLABINFO_NMI_CRASH",
            "plugin_name": "NMI enabled on VMWare guest may cause system crash",
            "ansible": 1,
            "rec_impact": 3,
            "rec_likelihood": 1,
            "resolution_risk": 2,
            "acked": false
        },
        {
            "rule_id": "vmware_guest_clock_unstable|VMWARE_GUEST_CLOCK_UNSTABLE",
            "description": "Failure to synchronize system clock with remote NTP servers on VMWare guest due to a bug in kernel",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "VMWare guests fail to synchronize system clock with remote NTP servers due to a bug in kernel.\n",
            "summary_html": "<p>VMWare guests fail to synchronize system clock with remote NTP servers due to a bug in kernel.</p>\n",
            "plugin": "vmware_guest_clock_unstable",
            "error_key": "VMWARE_GUEST_CLOCK_UNSTABLE",
            "plugin_name": "Failure to synchronize system clock with remote NTP servers on VMWare guest due to a bug in kernel",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "unsupported_journal_mode|UNSUPPORTED_JOURNAL_MODE_WARN",
            "description": "Risk of filesystem corruption when unsupported journal mode will be used in the next restart",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of \"ordered\".\n",
            "summary_html": "<p>Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of &quot;ordered&quot;.</p>\n",
            "plugin": "unsupported_journal_mode",
            "error_key": "UNSUPPORTED_JOURNAL_MODE_WARN",
            "plugin_name": "Filesystem corruption when using unsupported journal modes",
            "ansible": 1,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "unsupported_journal_mode|UNSUPPORTED_JOURNAL_MODE_ERROR_3",
            "description": "Risk of filesystem corruption when unsupported journal mode is being used in the host",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of \"ordered\".\n",
            "summary_html": "<p>Filesystem corruption might occur when using unsupported journal modes. Red Hat currently only supports the default journaling method of &quot;ordered&quot;.</p>\n",
            "plugin": "unsupported_journal_mode",
            "error_key": "UNSUPPORTED_JOURNAL_MODE_ERROR_3",
            "plugin_name": "Filesystem corruption when using unsupported journal modes",
            "ansible": 0,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        },
        {
            "rule_id": "udev_bug|UDEV_BUG",
            "description": "Udev rules fail to set correct ownership on devices during reboot when using specific versions of udev",
            "category": "Availability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "The `udev-147-2.73.el6_8.1` package has a bug that fails to set the correct ownership on devices on reboot.",
            "summary_html": "<p>The <code>udev-147-2.73.el6_8.1</code> package has a bug that fails to set the correct ownership on devices on reboot.</p>\n",
            "plugin": "udev_bug",
            "error_key": "UDEV_BUG",
            "plugin_name": "Udev rules fail to set correct ownership on devices during reboot when using specific versions of udev",
            "ansible": 1,
            "rec_impact": 1,
            "rec_likelihood": 3,
            "resolution_risk": 1,
            "acked": false
        },
        {
            "rule_id": "turla_malware|TURLA_MALWARE_DETECTED",
            "description": "Compromised system by Turla malware",
            "category": "Security",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "This rule tests for the presence of [Turla malware](https://access.redhat.com/articles/1290883) suite characteristics on the host.",
            "summary_html": "<p>This rule tests for the presence of <a href=\"https://access.redhat.com/articles/1290883\">Turla malware</a> suite characteristics on the host.</p>\n",
            "plugin": "turla_malware",
            "error_key": "TURLA_MALWARE_DETECTED",
            "plugin_name": "Compromised system by Turla malware",
            "ansible": 0,
            "rec_impact": 3,
            "rec_likelihood": 2,
            "resolution_risk": 1,
            "acked": false
        },
        {
            "rule_id": "tune_haproxy_openstack|TUNE_HAPROXY_OPENSTACK",
            "description": "OpenStack APIs respond slowly when exceeding maximum number of MariaDB connections",
            "category": "Performance",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "In OpenStack highly available deployments, the APIs behind HAProxy might respond very slowly or fail if the controller node Maria DB max_connections are not tuned correctly.",
            "summary_html": "<p>In OpenStack highly available deployments, the APIs behind HAProxy might respond very slowly or fail if the controller node Maria DB max_connections are not tuned correctly.</p>\n",
            "plugin": "tune_haproxy_openstack",
            "error_key": "TUNE_HAPROXY_OPENSTACK",
            "plugin_name": "OpenStack APIs respond slowly when exceeding maximum number of MariaDB connections",
            "ansible": 0,
            "rec_impact": 2,
            "rec_likelihood": 2,
            "resolution_risk": 2,
            "acked": false
        },
        {
            "rule_id": "tsc_rhel612_hung|TSC_BUG_NO_PROPER_KERNEL_PARAMETER",
            "description": "System Experiencing Short due to bug in the C3/C6 state transition",
            "category": "Stability",
            "severity": "WARN",
            "hitCount": 0,
            "summary": "System Experiencing Short due to bug in the C3/C6 state transition.\n",
            "summary_html": "<p>System Experiencing Short due to bug in the C3/C6 state transition.</p>\n",
            "plugin": "tsc_rhel612_hung",
            "error_key": "TSC_BUG_NO_PROPER_KERNEL_PARAMETER",
            "plugin_name": "System Experiencing Short due to bug in the C3/C6 state transition",
            "ansible": 1,
            "rec_impact": 3,
            "rec_likelihood": 2,
            "resolution_risk": 3,
            "acked": false
        }
    ],
    "ruleBinding": "implicit",
    "hitCount": 78,
    "affectedSystemCount": 14,
    "alwaysShow": false,
    "id": 465,
    "title": "Medium Risk Actions",
    "summary": "Actions identified with a medium level of risk",
    "content": "Actions identified with a medium level of risk",
    "priority": 30,
    "listed": "never",
    "tag": null,
    "category": null,
    "severity": "WARN",
    "hidden": false,
    "slug": "medium-risk"
}